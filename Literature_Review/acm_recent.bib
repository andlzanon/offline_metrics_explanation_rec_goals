@inproceedings{10.1145/3631700.3665226,
author = {Afreen, Neda and Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Malloci, Francesca Maridina and Marras, Mirko and Martis, Andrea Giovanni},
title = {Learner-centered Ontology for Explainable Educational Recommendation},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665226},
doi = {10.1145/3631700.3665226},
abstract = {Ontologies form the core of knowledge graphs, which act as faithful, semantic-rich sources for training models in delivering explainable recommendations. These models learn to extract logical paths between learners and resources to be recommended within the knowledge graph, according to behavior- and content-based patterns. Extracted paths are then used not only to provide recommendations, but also to generate accompanying textual explanations. Despite the potential of this approach, current ontologies derived from the traditional learner-resource interaction data fall short in terms of richness from an educational perspective. Conversely, general-purpose ontologies, while comprehensive in educational aspects, are overly complex for recommendation tasks. Unfortunately, a suboptimal ontology might prevent to articulate reasoning paths, and thus explanations, relevant for learners within the knowledge graph. To counter this limitation, in this paper, we propose LOXER, a novel ontology designed to unlock learner-centered logical paths for explainable educational recommendation. Our design integrates insights from diverse sources, including feedback from a local co-design group of learners, observations from specialized traditional large-scale educational recommendation datasets, and connections with well-known vocabularies of other existing ontologies. To validate our ontology, we conducted an evaluation of the explanation types it enables, involving university and lifelong learners and assessing explanation properties like effectiveness, decision-making speed, motivation, satisfaction, and confidence. Results show our ontology’s ability to foster diverse considerations during the learners’ decision-making process and to establish a semantic structure for knowledge graphs for explainable recommendation.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {567–575},
numpages = {9},
keywords = {Explainability., Ontology, Recommendation},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3637528.3671781,
author = {Zhang, Jingsen and Tang, Jiakai and Chen, Xu and Yu, Wenhui and Hu, Lantao and Jiang, Peng and Li, Han},
title = {Natural Language Explainable Recommendation with Robustness Enhancement},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671781},
doi = {10.1145/3637528.3671781},
abstract = {Natural language explainable recommendation has become a promising direction to facilitate more efficient and informed user decisions. Previous models mostly focus on how to enhance the explanation accuracy. However, the robustness problem has been largely ignored, which requires the explanations generated for similar user-item pairs should not be too much different. Different from traditional classification problems, improving the robustness of natural languages has two unique characteristics: (1) Different token importances, that is, different tokens play various roles in representing the complete sentence, and the robustness requirements for predicting them should also be different. (2) Continuous token semantics, that is, the similarity of the output should be judged based on semantics, and the sequences without any token-level overlap may also be highly similar. Based on these characteristics, we formulate and solve a novel problem in the recommendation domain, that is, robust natural language explainable recommendation. To the best of our knowledge, it is the first time in this field. Specifically, we base our modeling on adversarial robust optimization and design four types of heuristic methods to modify the adversarial outputs with weighted token probabilities and synonym replacements. Furthermore, to consider the mutual influence between the above characteristics, we regard language generation as a decision-making problem and design a dual-policy reinforcement learning framework to improve the robustness of the generated languages. We conduct extensive experiments to demonstrate the effectiveness of our framework.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4203–4212},
numpages = {10},
keywords = {adversarial learning, explainable recommendation, natural language explanations},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3640457.3688069,
author = {Ariza-Casabona, Alejandro and Boratto, Ludovico and Salam\'{o}, Maria},
title = {A Comparative Analysis of Text-Based Explainable Recommender Systems},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688069},
doi = {10.1145/3640457.3688069},
abstract = {One way to increase trust among users towards recommender systems is to provide the recommendation along with a textual explanation. In the literature, extraction-based, generation-based, and, more recently, hybrid solutions based on retrieval-augmented generation have been proposed to tackle the problem of text-based explainable recommendation. However, the use of different datasets, preprocessing steps, target explanations, baselines, and evaluation metrics complicates the reproducibility and state-of-the-art assessment of previous work among different model categories for successful advancements in the field. Our aim is to provide a comprehensive analysis of text-based explainable recommender systems by setting up a well-defined benchmark that accommodates generation-based, extraction-based, and hybrid approaches. Also, we enrich the existing evaluation of explainability and text quality of the explanations with a novel definition of feature hallucination. Our experiments on three real-world datasets unveil hidden behaviors and confirm several claims about model patterns. Our source code and preprocessed datasets are available at https://github.com/alarca94/text-exp-recsys24.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {105–115},
numpages = {11},
keywords = {Explainable Recommendation, Feature Hallucination, Natural Language Explanations, Reproducibility},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3543507.3583260,
author = {Zhang, Jingsen and Chen, Xu and Tang, Jiakai and Shao, Weiqi and Dai, Quanyu and Dong, Zhenhua and Zhang, Rui},
title = {Recommendation with Causality enhanced Natural Language Explanations},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583260},
doi = {10.1145/3543507.3583260},
abstract = {Explainable recommendation has recently attracted increasing attention from both academic and industry communities. Among different explainable strategies, generating natural language explanations is an important method, which can deliver more informative, flexible and readable explanations to facilitate better user decisions. Despite the effectiveness, existing models are mostly optimized based on the observed datasets, which can be skewed due to the selection or exposure bias. To alleviate this problem, in this paper, we formulate the task of explainable recommendation with a causal graph, and design a causality enhanced framework to generate unbiased explanations. More specifically, we firstly define an ideal unbiased learning objective, and then derive a tractable loss for the observational data based on the inverse propensity score (IPS), where the key is a sample re-weighting strategy for equalizing the loss and ideal objective in expectation. Considering that the IPS estimated from the sparse and noisy recommendation datasets can be inaccurate, we introduce a fault tolerant mechanism by minimizing the maximum loss induced by the sample weights near the IPS. For more comprehensive modeling, we further analyze and infer the potential latent confounders induced by the complex and diverse user personalities. We conduct extensive experiments by comparing with the state-of-the-art methods based on three real-world datasets to demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {876–886},
numpages = {11},
keywords = {Explainable Recommendation, Natural Language Explanations},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3624918.3625331,
author = {Yu, Yi and Sugiyama, Kazunari and Jatowt, Adam},
title = {AdaReX: Cross-Domain, Adaptive, and Explainable Recommender System},
year = {2023},
isbn = {9798400704086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624918.3625331},
doi = {10.1145/3624918.3625331},
abstract = {Explainability is an inherent issue of recommender systems and has received a lot of attention recently. Generative explainable recommendation, which provides personalized explanations by generating textual rationales, is emerging as an effective solution. Despite promising, current methods face limitations in their reliance on dense training data, which hinders the generalizability of explainable recommender systems. Our work tackles a novel problem of cross-domain explainable recommendation aiming to extend the generalizability of explainable recommender systems. To solve this, we propose a novel approach that models aspects extracted from past reviews, to empower the explainable recommender systems by leveraging knowledge from other domains. Specifically, we propose AdaReX (Adaptive eXplainable Recommendation), to model auxiliary and target domains simultaneously. By performing specific tasks in respective domains and their interconnection via a discriminator model, AdaReX allows the aspect sequences to learn common knowledge across different domains and tasks. Furthermore, through our proposed optimization objective, the learning of aspect sequence is deeply cross-interacted with in-domain users and items’ latent factors, enabling the enhanced sharing of knowledge between domains. Our extensive experiments on real datasets demonstrate that our approach not only generates better explanations and recommendations for sparse users but also improves performance for general users.},
booktitle = {Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region},
pages = {272–281},
numpages = {10},
keywords = {Explainable Recommender System, Natural Language Generation},
location = {Beijing, China},
series = {SIGIR-AP '23}
}

@inproceedings{10.1145/3485447.3511937,
author = {Geng, Shijie and Fu, Zuohui and Tan, Juntao and Ge, Yingqiang and de Melo, Gerard and Zhang, Yongfeng},
title = {Path Language Modeling over Knowledge Graphsfor Explainable Recommendation},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511937},
doi = {10.1145/3485447.3511937},
abstract = {To facilitate human decisions with credible suggestions, personalized recommender systems should have the ability to generate corresponding explanations while making recommendations. Knowledge graphs (KG), which contain comprehensive information about users and products, are widely used to enable this. By reasoning over a KG in a node-by-node manner, existing explainable models provide a KG-grounded path for each user-recommended item. Such paths serve as an explanation and reflect the historical behavior pattern of the user. However, not all items can be reached following the connections within the constructed KG under finite hops. Hence, previous approaches are constrained by a recall bias in terms of existing connectivity of KG structures. To overcome this, we propose a novel Path Language Modeling Recommendation (PLM-Rec) framework, learning a language model over KG paths consisting of entities and edges. Through path sequence decoding, PLM-Rec unifies recommendation and explanation in a single step and fulfills them simultaneously. As a result, PLM-Rec not only captures the user behaviors but also eliminates the restriction to pre-existing KG connections, thereby alleviating the aforementioned recall bias. Moreover, the proposed technique makes it possible to conduct explainable recommendation even when the KG is sparse or possesses a large number of relations. Experiments and extensive ablation studies on three Amazon e-commerce datasets demonstrate the effectiveness and explainability of the PLM-Rec framework.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {946–955},
numpages = {10},
keywords = {Explainable Recommendation, Knowledge Graph, Path Language Model, Recall Bias, Recommender Systems},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3640457.3688075,
author = {Zhang, Xiaoyu and Li, Yishan and Wang, Jiayin and Sun, Bowen and Ma, Weizhi and Sun, Peijie and Zhang, Min},
title = {Large Language Models as Evaluators for Recommendation Explanations},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688075},
doi = {10.1145/3640457.3688075},
abstract = {The explainability of recommender systems has attracted significant attention in academia and industry. Many efforts have been made for explainable recommendations, yet evaluating the quality of the explanations remains a challenging and unresolved issue. In recent years, leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning. However, evaluating recommendation explanatory texts is different from these NLG tasks, as its criteria are related to human perceptions and are usually subjective. In this paper, we investigate whether LLMs can serve as evaluators of recommendation explanations. To answer the question, we utilize real user feedback on explanations given from previous work and additionally collect third-party annotations and LLM evaluations. We design and apply a 3-level meta-evaluation strategy to measure the correlation between evaluator labels and the ground truth provided by users. Our experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings. We also provide further insights into combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations. Our study verifies that utilizing LLMs as evaluators can be an accurate, reproducible and cost-effective solution for evaluating recommendation explanation texts. Our code is available here1.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {33–42},
numpages = {10},
keywords = {Evaluation, Explainable Recommendation, Large Language Model},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3485447.3512029,
author = {Pan, Sicheng and Li, Dongsheng and Gu, Hansu and Lu, Tun and Luo, Xufang and Gu, Ning},
title = {Accurate and Explainable Recommendation via Review Rationalization},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512029},
doi = {10.1145/3485447.3512029},
abstract = {Auxiliary information, such as reviews, have been widely adopted to improve collaborative filtering (CF) algorithms, e.g., to boost the accuracy and provide explanations. However, most of the existing methods cannot distinguish between co-appearance and causality when learning from the reviews, so that they may rely on spurious correlations rather than causal relations in the recommendation — leading to poor generalization performance and unconvincing explanations. In this paper, we propose a Recommendation via Review Rationalization (R3) method including 1) a rationale generator to extract rationales from reviews to alleviate the effects of spurious correlations; 2) a rationale predictor to predict user ratings on items only from generated rationales; and 3) a correlation predictor upon both rationales and correlational features to ensure conditional independence between spurious correlations and rating predictions given causal rationales. Extensive experiments on real-world datasets show that the proposed method can achieve better generalization performance than state-of-the-art CF methods and provide causal-aware explanations even when the test data distribution changes.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3092–3101},
numpages = {10},
keywords = {explainability, rationalization, recommendation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3604915.3609491,
author = {B\"{o}lz, Felix and Nurbakova, Diana and Calabretto, Sylvie and Gerl, Armin and Brunie, Lionel and Kosch, Harald},
title = {HUMMUS: A Linked, Healthiness-Aware, User-centered and Argument-Enabling Recipe Data Set for Recommendation},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3609491},
doi = {10.1145/3604915.3609491},
abstract = {The overweight and obesity rate is increasing for decades worldwide. Healthy nutrition is, besides education and physical activity, one of the various keys to tackle this issue. In an effort to increase the availability of digital, healthy recommendations, the scientific area of food recommendation extends its focus from the accuracy of the recommendations to beyond-accuracy goals like transparency and healthiness. To address this issue a data basis is required, which in the ideal case encompasses user-item interactions like ratings and reviews, food-related information such as recipe details, nutritional data, and in the best case additional data which describes the food items and their relations semantically. Though several recipe recommendation data sets exist, to the best of our knowledge, a holistic large-scale healthiness-aware and connected data sets have not been made available yet. The lack of such data could partially explain the poor popularity of the topic of healthy food recommendation when compared to the domain of movie recommendation. In this paper, we show that taking into account only user-item interactions is not sufficient for a recommendation. To close this gap, we propose a connected data set called HUMMUS (Health-aware User-centered recoMMendation and argUment-enabling data Set) collected from Food.com containing multiple features including rich nutrient information, text reviews, and ratings, enriched by the authors with extra features such as Nutri-scores and connections to semantic data like the FoodKG and the FoodOn ontology. We hope that these data will contribute to the healthy food recommendation domain.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1–11},
numpages = {11},
keywords = {Explainable recommendation, Healthiness-aware recommendation, Knowledge graph, Nutrition scores, Recipe data set},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3539618.3591884,
author = {Guo, Shuyu and Zhang, Shuo and Sun, Weiwei and Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun},
title = {Towards Explainable Conversational Recommender Systems},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591884},
doi = {10.1145/3539618.3591884},
abstract = {Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in CRS, we propose ten evaluation perspectives based on the concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approaches to perform explanation generation based on E-ReDial. Experimental results suggest that models trained on E-ReDial can significantly improve explainability while introducing knowledge into the models can further improve the performance. GPT-3 in the in-context learning setting can generate more realistic and diverse movie descriptions. In contrast, T5 training on E-Redial can better generate clear reasons for recommendations based on user preferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2786–2795},
numpages = {10},
keywords = {conversational information access, conversational recommendation, explainable recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3636555.3636898,
author = {Frej, Jibril and Shah, Neel and Knezevic, Marta and Nazaretsky, Tanya and K\"{a}ser, Tanja},
title = {Finding Paths for Explainable MOOC Recommendation: A Learner Perspective},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636898},
doi = {10.1145/3636555.3636898},
abstract = {The increasing availability of Massive Open Online Courses (MOOCs) has created a necessity for personalized course recommendation systems. These systems often combine neural networks with Knowledge Graphs (KGs) to achieve richer representations of learners and courses. While these enriched representations allow more accurate and personalized recommendations, explainability remains a significant challenge which is especially problematic for certain domains with significant impact such as education and online learning. Recently, a novel class of recommender systems that uses reinforcement learning and graph reasoning over KGs has been proposed to generate explainable recommendations in the form of paths over a KG. Despite their accuracy and interpretability on e-commerce datasets, these approaches have scarcely been applied to the educational domain and their use in practice has not been studied. In this work, we propose an explainable recommendation system for MOOCs that uses graph reasoning. To validate the practical implications of our approach, we conducted a user study examining user perceptions of our new explainable recommendations. We demonstrate the generalizability of our approach by conducting experiments on two educational datasets: COCO and Xuetang.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {426–437},
numpages = {12},
keywords = {Explainable AI, MOOCs, Recommendation, User study},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3616855.3635855,
author = {Liu, Xu and Yu, Tong and Xie, Kaige and Wu, Junda and Li, Shuai},
title = {Interact with the Explanations: Causal Debiased Explainable Recommendation System},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635855},
doi = {10.1145/3616855.3635855},
abstract = {In recent years, the field of recommendation systems has witnessed significant advancements, with explainable recommendation systems gaining prominence as a crucial area of research. These systems aim to enhance user experience by providing transparent and compelling recommendations, accompanied by explanations. However, a persistent challenge lies in addressing biases that can influence the recommendations and explanations offered by these systems. Such biases often stem from a tendency to favor popular items and generate explanations that highlight their common attributes, thereby deviating from the objective of delivering personalized recommendations and explanations. While existing debiasing methods have been applied in explainable recommendation systems, they often overlook the model-generated explanations in tackling biases. Consequently, biases in model-generated explanations may persist, potentially compromising system performance and user satisfaction.To address biases in both model-generated explanations and recommended items, we discern the impact of model-generated explanations in recommendation through a formulated causal graph. Inspired by this causal perspective, we propose a novel approach termed Causal Explainable Recommendation System (CERS), which incorporates model-generated explanations into the debiasing process and enacts causal interventions based on user feedback on the explanations. By utilizing model-generated explanations as intermediaries between user-item interactions and recommendation results, we adeptly mitigate the biases via targeted causal interventions. Experimental results demonstrate the efficacy of CERS in reducing popularity bias while simultaneously improving recommendation performance, leading to more personalized and tailored recommendations. Human evaluation further affirms that CERS generates explanations tailored to individual users, thereby enhancing the persuasiveness of the system.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {472–481},
numpages = {10},
keywords = {causal reasoning, debiased recommendation, explainable recommendation system},
location = {Merida, Mexico},
series = {WSDM '24}
}

@inproceedings{10.1145/3477495.3531873,
author = {Radlinski, Filip and Balog, Krisztian and Diaz, Fernando and Dixon, Lucas and Wedin, Ben},
title = {On Natural Language User Profiles for Transparent and Scrutable Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531873},
doi = {10.1145/3477495.3531873},
abstract = {Natural interaction with recommendation and personalized search systems has received tremendous attention in recent years. We focus on the challenge of supporting people's understanding and control of these systems and explore a fundamentally new way of thinking about representation of knowledge in recommendation and personalization systems. Specifically, we argue that it may be both desirable and possible for algorithms that use natural language representations of users' preferences to be developed. We make the case that this could provide significantly greater transparency, as well as affordances for practical actionable interrogation of, and control over, recommendations. Moreover, we argue that such an approach, if successfully applied, may enable a major step towards systems that rely less on noisy implicit observations while increasing portability of knowledge of one's interests.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2863–2874},
numpages = {12},
keywords = {natural language, recommendation, scrutability, transparency},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3539618.3591776,
author = {Shuai, Jie and Wu, Le and Zhang, Kun and Sun, Peijie and Hong, Richang and Wang, Meng},
title = {Topic-enhanced Graph Neural Networks for Extraction-based Explainable Recommendation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591776},
doi = {10.1145/3539618.3591776},
abstract = {Review information has been demonstrated beneficial for the explainable recommendation. It can be treated as training corpora for generation-based methods or knowledge bases for extraction-based models. However, for generation-based methods, the sparsity of user-generated reviews and the high complexity of generative language models lead to a lack of personalization and adaptability. For extraction-based methods, focusing only on relevant attributes makes them invalid in situations where explicit attribute words are absent, limiting the potential of extraction-based models.To this end, in this paper, we focus on the explicit and implicit analysis of review information simultaneously and propose novel a Topic-enhanced Graph Neural Networks (TGNN) to fully explore review information for better explainable recommendations. To be specific, we first use a pre-trained topic model to analyze reviews at the topic level, and design a sentence-enhanced topic graph to model user preference explicitly, where topics are intermediate nodes between users and items. Corresponding sentences serve as edge features. Thus, the requirement of explicit attribute words can be mitigated. Meanwhile, we leverage a review-enhanced rating graph to model user preference implicitly, where reviews are also considered as edge features for fine-grained user-item interaction modeling. Next, user and item representations from two graphs are used for final rating prediction and explanation extraction. Extensive experiments on three real-world datasets demonstrate the superiority of our proposed TGNN with both recommendation accuracy and explanation quality.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1188–1197},
numpages = {10},
keywords = {explainable recommendation, graph neural network, review-based recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3617023.3617052,
author = {Suarez Mariscal, Claudia and de Lima, Bruno Santana Massena and Galante, Renata and Cordeiro, Weverton},
title = {Assessing Explainable Recommendations from Knowledge Graph-based in an International Streaming Platform},
year = {2023},
isbn = {9798400709081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617023.3617052},
doi = {10.1145/3617023.3617052},
abstract = {Explainable recommendations can increase users’ confidence in the results provided by recommendation systems by providing justifications of why a certain item is recommended. In this way, the use of the Knowledge Graph (KG) guarantees an optimal organization of the data enabling one to trace the relationships between entities (users, recommended items, item attributes and features, and so on). Current proposals use different approaches such as embedding, connection, and propagation to deal with common problems that persist when generating recommendations, such as cold start or data lake. However, the complexity of recommendation models seems to increase when there is a large amount of data. In this work, we propose an analysis of the applicability of different frameworks based on knowledge graphs to obtain explanatory recommendations using a large dataset from an international streaming platform, with the idea of knowing the advantages and limitations of each approach to validate if complex models should really be used to obtain the best results. Through the experimentation of RippleNet, KGCN, KGAT, ECFKG, and DSKE, we focus on dataset structure, category-based, and refinement type of each framework. To conclude, we provide details on some general points of the evaluation of all frameworks using our dataset.},
booktitle = {Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
pages = {213–220},
numpages = {8},
keywords = {Connection-based model, Embedding-based model, Explainable recommendation, Knowledge Graph, Propagation-based model},
location = {Ribeir\~{a}o Preto, Brazil},
series = {WebMedia '23}
}

@Comment{jabref-meta: databaseType:bibtex;}
