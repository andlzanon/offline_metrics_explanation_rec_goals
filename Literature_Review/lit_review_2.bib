@Article{Rana2022,
  author    = {Rana, Arpit and D’Addio, Rafael M. and Manzato, Marcelo G. and Bridge, Derek},
  journal   = {User Modeling and User-Adapted Interaction},
  title     = {Extended recommendation-by-explanation},
  year      = {2022},
  issn      = {0924-1868},
  number    = {1-2},
  pages     = {91–131},
  volume    = {32},
  doi       = {10.1007/s11257-021-09317-4},
  publisher = {User Modeling and User-Adapted Interaction},
}

@Article{Haque2025,
  author    = {Haque, Akm Bahalul and Islam, Najmul and Mikalef, Patrick},
  journal   = {Electronic Markets},
  title     = {To Explain or Not To Explain: An Empirical Investigation of AI-based Recommendations on Social Media Platforms},
  year      = {2025},
  issn      = {1019-6781},
  number    = {1},
  volume    = {35},
  doi       = {10.1007/s12525-024-00741-z},
  publisher = {Electronic Markets},
}

@Article{Ranjbar2024,
  author    = {Ranjbar, Niloofar and Momtazi, Saeedeh and Homayoonpour, Mohammadmehdi},
  journal   = {Machine Learning},
  title     = {Explaining recommendation system using counterfactual textual explanations},
  year      = {2024},
  issn      = {0885-6125},
  number    = {4},
  pages     = {1989–2012},
  volume    = {113},
  doi       = {10.1007/s10994-023-06390-1},
  publisher = {Machine Learning},
}

@InBook{CaroMartinez2024,
  author    = {Caro-Martínez, Marta and Jorro-Aragoneses, José L. and Díaz-Agudo, Belén and Recio-García, Juan A.},
  pages     = {28–41},
  publisher = {Communications in Computer and Information Science},
  title     = {Graph-Based Interface for Explanations by Examples in Recommender Systems: A User Study},
  year      = {2024},
  booktitle = {Communications in Computer and Information Science},
  doi       = {10.1007/978-3-031-63797-1_2},
  issn      = {1865-0929},
}

@Article{DeCampos2024,
  author    = {De Campos, Luis M. and Fernández-Luna, Juan M. and Huete, Juan F.},
  journal   = {User Modeling and User-Adapted Interaction},
  title     = {An explainable content-based approach for recommender systems: a case study in journal recommendation for paper submission},
  year      = {2024},
  issn      = {0924-1868},
  number    = {4},
  pages     = {1431–1465},
  volume    = {34},
  doi       = {10.1007/s11257-024-09400-6},
  publisher = {User Modeling and User-Adapted Interaction},
}

@Article{xie_wang_xu_chen_zheng_tang_2024,
  author    = {Xie, Fenfang and Wang, Yuansheng and Xu, Kun and Chen, Liang and Zheng, Zibin and Tang, Mingdong},
  journal   = {IEEE Transactions on Computational Social Systems},
  title     = {A Review-Level Sentiment Information Enhanced Multitask Learning Approach for Explainable Recommendation},
  year      = {2024},
  issn      = {2329-924X},
  number    = {5},
  pages     = {5925–5934},
  volume    = {11},
  doi       = {10.1109/tcss.2024.3376728},
  publisher = {IEEE Transactions on Computational Social Systems},
}

@InProceedings{le_abel_gouspillou_2023,
  author  = {Le, Ngoc Luyen and Abel, Marie-Hélène and Gouspillou, Philippe},
  title   = {Combining Embedding-Based and Semantic-Based Models for Post-Hoc Explanations in Recommender Systems},
  year    = {2023},
  pages   = {4619–4624},
  doi     = {10.1109/smc53992.2023.10394410},
  journal = {IEEE International Conference on Systems, Man and Cybernetics},
}

@InProceedings{bastola_shakya_2024,
  author  = {Bastola, Rama and Shakya, Subarna},
  title   = {Knowledge-Enriched Graph Convolution Network for Hybrid Explainable Recommendation from Review Texts and Reasoning Path},
  year    = {2024},
  pages   = {590–599},
  doi     = {10.1109/icict60155.2024.10544384},
  journal = {International Conference on Inventive Computation Technologies},
}

@Article{hu_liu_miao_lin_miao_2022,
  author    = {Hu, Yidan and Liu, Yong and Miao, Chunyan and Lin, Gongqi and Miao, Yuan},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {Aspect-guided Syntax Graph Learning for Explainable Recommendation},
  year      = {2022},
  issn      = {1041-4347},
  pages     = {1–14},
  doi       = {10.1109/tkde.2022.3221847},
  publisher = {IEEE Transactions on Knowledge and Data Engineering},
}

@InProceedings{zhan_li_li_liu_gupta_kot_2023,
  author    = {Zhan, Huijing and Li, Ling and Li, Shaohua and Liu, Weide and Gupta, Manas and Kot, Alex C.},
  title     = {Towards Explainable Recommendation Via Bert-Guided Explanation Generator},
  year      = {2023},
  pages     = {1–5},
  publisher = {International Conference on Acoustics, Speech, and Signal Processing},
  doi       = {10.1109/icassp49357.2023.10096389},
  journal   = {International Conference on Acoustics, Speech, and Signal Processing},
}

@Article{LI2025110542,
  author   = {Ying Li and Ming Li and Jin Ding and Yixue Bai},
  journal  = {Engineering Applications of Artificial Intelligence},
  title    = {Two-layer knowledge graph transformer network-based question and answer explainable recommendation},
  year     = {2025},
  issn     = {0952-1976},
  pages    = {110542},
  volume   = {149},
  abstract = {The question and answer (Q&A) recommendation in community question answering (CQA) helps users quickly and accurately find the desired Q&A. However, existing studies face the problems of sparse interaction data, cold starts, and a lack of explanations. This paper proposes a novel Q&A explainable recommendation approach based on a two-layer knowledge graph transformer network. It alleviates the sparse data and cold start problem by the novel two-layer knowledge graph. First, a two-layer knowledge graph in CQA is constructed. The interaction layer helps to enrich the associations between users and questions and answers (Q&As). The semantic layer provides semantic associations and reflects contextual domain knowledge. Second, a critical meta-path recognition module is constructed to learn the critical meta-paths between users and documents from the interaction layer. Then, a user and Q&A embedding method based on a two-layer knowledge graph is proposed to enhance the user and Q&A representations. Finally, a recommendation and explanation layer is established to obtain personalized Q&A recommendation results and corresponding explanations. Compared with the baselines, the proposed method shows superior performance. It achieves average improvements of 21.28%, 28.41% and 27.18% in precision, recall and F1-measure, respectively, in the top-K Q&A recommendation separately. It improves the area under the curve and F1-measure of the click-through rate prediction recommendation by 11.32% and 23.06%, respectively.},
  doi      = {https://doi.org/10.1016/j.engappai.2025.110542},
  keywords = {Question and answer recommendation, Knowledge graph-based recommendation, Two-layer knowledge graph, Explainable recommendation},
  url      = {https://www.sciencedirect.com/science/article/pii/S0952197625005421},
}

@Article{XIE2021235,
  author   = {Jin Xie and Fuxi Zhu and Xuefei Li and Sheng Huang and Shichao Liu},
  journal  = {Neurocomputing},
  title    = {Attentive preference personalized recommendation with sentence-level explanations},
  year     = {2021},
  issn     = {0925-2312},
  pages    = {235-247},
  volume   = {426},
  abstract = {Personalized recommendation mostly employs users’ historical data to improve their user profiles, and these profiles are then used as the bases for recommendations. Because reviews can contain a large amount of information regarding user preferences and item features, they can be naturally into recommender systems (RSs) as contextual information, thus solving the problem of data sparsity and helping to provide personalized recommendations. The existing technology mainly extracts latent representations of users or items in an independent and static manner. We argue that static embedding cannot fully capture a user's preferences. Indeed, a user will have different preferences corresponding to different items. This type of review-based recommendation model cannot provide a personalized, and complete semantic explanation of a candidate recommendation item to a user. In this paper, we introduce an attention mechanism to explore the importance of specific sentences in reviews for different users and propose a novel attentive preference personalized recommendation with sentence-level explanations (APSE). The APSE employs the latent features of users and items and the latent factors of their pairwise interactions to obtain review representations. Then, the APSE uses probability matrix factorization to model additional high-level feature interactions based on these user-item pairs for rating prediction. We implement review feature learning in the APSE to exploit review data in which an attentive mechanism is used to highlight the influences of words and sentences to achieve focused paragraph embedding. Finally, the APSE also employs an explanation sentence judgment mechanism that implements the user-item pair interaction method to extract comments or statements that pertain to user preferences as recommendation interpretations. Experiments are performed on real-world datasets for validation. Additionally, we show the important words and sentences highlighted by the attentive mechanism. At the end of the experiment, a specific item explanation for a user is produced and compared with the user's existing comments. The results show that the performance of the APSE can exceed that of various recommended models when the available ratings are limited.},
  doi      = {https://doi.org/10.1016/j.neucom.2020.10.041},
  keywords = {Explainable recommendation, Review representation, Attentive mechanism},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231220315903},
}

@Article{balloccu_boratto_fenu_marras_2022,
  author    = {Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Marras, Mirko},
  journal   = {Software Impacts},
  title     = {XRecSys: A framework for path reasoning quality in explainable recommendation},
  year      = {2022},
  issn      = {2665-9638},
  pages     = {100404},
  volume    = {14},
  doi       = {10.1016/j.simpa.2022.100404},
  publisher = {Software Impacts},
}

@Article{balloccu_boratto_fenu_marras_2023,
  author    = {Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Marras, Mirko},
  journal   = {Knowledge-Based Systems},
  title     = {Reinforcement recommendation reasoning through knowledge graphs for explanation path quality},
  year      = {2023},
  issn      = {0950-7051},
  pages     = {110098},
  volume    = {260},
  doi       = {10.1016/j.knosys.2022.110098},
  publisher = {Knowledge-Based Systems},
}

@Article{WU2024111133,
  author   = {Huiqiong Wu and Guibing Guo and Enneng Yang and Yudong Luo and Yabo Chu and Linying Jiang and Xingwei Wang},
  journal  = {Knowledge-Based Systems},
  title    = {PESI: Personalized Explanation recommendation with Sentiment Inconsistency between ratings and reviews},
  year     = {2024},
  issn     = {0950-7051},
  pages    = {111133},
  volume   = {283},
  abstract = {Explainable recommendations aim to generate personalized explanations for suggested items, which are provided based on the historical interactions (e.g., ratings) between users and items. Review contents are often taken as a proxy of explanations. However, most review-based models presume a sentiment consistency between user ratings and review contents, ignoring their inconsistency in real applications. By analyzing three real datasets, we observe that a user may share a positive (negative) opinion to an item in terms of rating value but a negative (positive) sentiment in terms of review content, and such contradicting scenario takes over 40% of all cases in general. To resolve this issue, in this paper we propose a novel explainable recommendation model called PESI, which can generate accurate Personalized Explanations recommendation with the involvement of Sentiment Inconsistency between ratings and reviews. Specifically, PESI consists of three modules: rating prediction, explanation generation, and a novel rating-review inconsistency extraction. The inconsistency extraction module disentangles ratings and reviews, effectively distinguishing both shared and private features, and ensuring accurate disentanglement through contrastive learning objectives. Then, the extracted inconsistent features are injected into the explanation generation module to provide more personalized and higher-quality explanations. The experimental results on the three datasets show that PESI consistently outperforms other competing methods in terms of explanation quality.},
  doi      = {https://doi.org/10.1016/j.knosys.2023.111133},
  keywords = {Recommendation system, Explanation generation, Sentiment analysis},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950705123008833},
}

@InProceedings{10.1145/2766462.2767755,
  author    = {McAuley, Julian and Targett, Christopher and Shi, Qinfeng and van den Hengel, Anton},
  booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  title     = {Image-Based Recommendations on Styles and Substitutes},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {43–52},
  publisher = {Association for Computing Machinery},
  series    = {SIGIR '15},
  abstract  = {Humans inevitably develop a sense of the relationships between objects, some of which are based on their appearance. Some pairs of objects might be seen as being alternatives to each other (such as two pairs of jeans), while others may be seen as being complementary (such as a pair of jeans and a matching shirt). This information guides many of the choices that people make, from buying clothes to their interactions with each other. We seek here to model this human sense of the relationships between objects based on their appearance. Our approach is not based on fine-grained modeling of user annotations but rather on capturing the largest dataset possible and developing a scalable method for uncovering human notions of the visual relationships within. We cast this as a network inference problem defined on graphs of related images, and provide a large-scale dataset for the training and evaluation of the same. The system we develop is capable of recommending which clothes and accessories will go well together (and which will not), amongst a host of other applications.},
  doi       = {10.1145/2766462.2767755},
  isbn      = {9781450336215},
  keywords  = {visual features, recommender systems, metric learning},
  location  = {Santiago, Chile},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2766462.2767755},
}

@InProceedings{10.1145/3340531.3411992,
  author    = {Li, Lei and Zhang, Yongfeng and Chen, Li},
  booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  title     = {Generate Neural Template Explanations for Recommendation},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {755–764},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '20},
  abstract  = {Personalized recommender systems are important to assist user decision-making in the era of information overload. Meanwhile, explanations of the recommendations further help users to better understand the recommended items so as to make informed choices, which gives rise to the importance of explainable recommendation research. Textual sentence-based explanation has been an important form of explanations for recommender systems due to its advantage in communicating rich information to users. However, current approaches to generating sentence explanations are either limited to predefined sentence templates, which restricts the sentence expressiveness, or opt for free-style sentence generation, which makes it difficult for sentence quality control. In an attempt to benefit both sentence expressiveness and quality, we propose a Neural Template (NETE) explanation generation framework, which brings the best of both worlds by learning sentence templates from data and generating template-controlled sentences that comment about specific features. Experimental results on real-world datasets show that NETE consistently outperforms state-of-the-art explanation generation approaches in terms of sentence quality and expressiveness. Further analysis on case study also shows the advantages of NETE on generating diverse and controllable explanations.},
  doi       = {10.1145/3340531.3411992},
  isbn      = {9781450368599},
  keywords  = {explainable recommendation, natural language generation, neural template explanation, recommender systems},
  location  = {Virtual Event, Ireland},
  numpages  = {10},
  url       = {https://doi.org/10.1145/3340531.3411992},
}

@InProceedings{10.1145/3292500.3330989,
  author    = {Wang, Xiang and He, Xiangnan and Cao, Yixin and Liu, Meng and Chua, Tat-Seng},
  booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  title     = {KGAT: Knowledge Graph Attention Network for Recommendation},
  year      = {2019},
  address   = {New York, NY, USA},
  pages     = {950–958},
  publisher = {Association for Computing Machinery},
  series    = {KDD '19},
  abstract  = {To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism. We release the codes and datasets at https://github.com/xiangwang1223/knowledge_graph_attention_network.},
  doi       = {10.1145/3292500.3330989},
  isbn      = {9781450362016},
  keywords  = {recommendation, knowledge graph, higher-order connectivity, graph neural network, embedding propagation, collaborative filtering},
  location  = {Anchorage, AK, USA},
  numpages  = {9},
  url       = {https://doi.org/10.1145/3292500.3330989},
}

@InProceedings{10.1145/3351095.3372852,
  author    = {Zhang, Yunfeng and Liao, Q. Vera and Bellamy, Rachel K. E.},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  title     = {Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making},
  year      = {2020},
  address   = {New York, NY, USA},
  pages     = {295–305},
  publisher = {Association for Computing Machinery},
  series    = {FAT* '20},
  abstract  = {Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.},
  doi       = {10.1145/3351095.3372852},
  isbn      = {9781450369367},
  keywords  = {confidence, decision support, explainable AI, trust},
  location  = {Barcelona, Spain},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3351095.3372852},
}

@InProceedings{10.1145/2806416.2806504,
  author    = {He, Xiangnan and Chen, Tao and Kan, Min-Yen and Chen, Xiao},
  booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management},
  title     = {TriRank: Review-aware Explainable Recommendation by Modeling Aspects},
  year      = {2015},
  address   = {New York, NY, USA},
  pages     = {1661–1670},
  publisher = {Association for Computing Machinery},
  series    = {CIKM '15},
  abstract  = {Most existing collaborative filtering techniques have focused on modeling the binary relation of users to items by extracting from user ratings. Aside from users' ratings, their affiliated reviews often provide the rationale for their ratings and identify what aspects of the item they cared most about. We explore the rich evidence source of aspects in user reviews to improve top-N recommendation. By extracting aspects (i.e., the specific properties of items) from textual reviews, we enrich the user--item binary relation to a user--item--aspect ternary relation. We model the ternary relation as a heterogeneous tripartite graph, casting the recommendation task as one of vertex ranking. We devise a generic algorithm for ranking on tripartite graphs -- TriRank -- and specialize it for personalized recommendation. Experiments on two public review datasets show that it consistently outperforms state-of-the-art methods. Most importantly, TriRank endows the recommender system with a higher degree of explainability and transparency by modeling aspects in reviews. It allows users to interact with the system through their aspect preferences, assisting users in making informed decisions.},
  doi       = {10.1145/2806416.2806504},
  isbn      = {9781450337946},
  keywords  = {tripartite graph ranking, top-n recommendation, reviews, explanable recommendation, comments, aspects},
  location  = {Melbourne, Australia},
  numpages  = {10},
  url       = {https://doi.org/10.1145/2806416.2806504},
}

@InProceedings{10.1145/3485447.3512031,
  author    = {Yang, Aobo and Wang, Nan and Cai, Renqin and Deng, Hongbo and Wang, Hongning},
  booktitle = {Proceedings of the ACM Web Conference 2022},
  title     = {Comparative Explanations of Recommendations},
  year      = {2022},
  address   = {New York, NY, USA},
  pages     = {3113–3123},
  publisher = {Association for Computing Machinery},
  series    = {WWW '22},
  abstract  = {As recommendation is essentially a comparative (or ranking) process, a good explanation should illustrate to users why an item is believed to be better than another, i.e., comparative explanations about the recommended items. Ideally, after reading the explanations, a user should reach the same ranking of items as the system’s. Unfortunately, little research attention has yet been paid on such comparative explanations. In this work, we develop an extract-and-refine architecture to explain the relative comparisons among a set of ranked items from a recommender system. For each recommended item, we first extract one sentence from its associated reviews that best suits the desired comparison against a set of reference items. Then this extracted sentence is further articulated with respect to the target user through a generative model to better explain why the item is recommended. We design a new explanation quality metric based on BLEU to guide the end-to-end training of the extraction and refinement components, which avoids generation of generic content. Extensive offline evaluations on two large recommendation benchmark datasets and serious user studies against an array of state-of-the-art explainable recommendation algorithms demonstrate the necessity of comparative explanations and the effectiveness of our solution.},
  doi       = {10.1145/3485447.3512031},
  isbn      = {9781450390965},
  keywords  = {comparative explanation, explainable recommendation, extract-and-refine, text generation},
  location  = {Virtual Event, Lyon, France},
  numpages  = {11},
  url       = {https://doi.org/10.1145/3485447.3512031},
}

@Comment{jabref-meta: databaseType:bibtex;}
